### COMMON QUESTIONS
    
**How is a new process defined by the framework? Is the process going to be a grails service, a batch script, an R or Python script? Does it need to support all of them?**
    
Azure batch service has really clean abstractions regarding what to run and where to run it, by separating these two out we are able to virtually run any script given that the VM is provisioned to run that script. What to run is handled by "Applications", under batch account you can upload a zipped version of file you would want to execute. These applications can then be made available in the VM where they will be executed. 
For eg: if you want to run R scripts, you have to make sure that your VM has provisions to run Rscript. There are multiple way to to this:
a) whenever a VM starts, you can define a start task that would install the Rscript and dependent libraries that will enable you to ececute R scripts
b) create a VM image where the Rscript and libraries have already been installed and instantiate VM based on the image(or even docker images) so that you don't have to install them every time a VM instantiates

**How are the processing need defined? - # of parallel processes, input the the process, output handling, etc...**
This is controlled by your plugin and can be configured to run based on your needs. For eg : when you create a task you can also include the files needed as inputs to the application so the script can work on those file and generate some output. Each task upon succesfull completion or failure will copy the "working directory" to blob storage which can be accessed via plugin again. Or you can choose to call external APIs from the application, but the output mechanism is almost always same.
Also we have task events(running, complete, failed), so whenever you provide the hooks in youe plugin they will be in the consuming application.

**How is the framework integrated into the service? - Does each service have to write code to integrate the framework or is there going to be a library/plugin provided to enable integration**
We have a plugin that you should be able to use in any java application

**What are the interfaces exposed by the framework to track the processing?**
Look into BatchJobHandler Interface, which should help you get an idea on the hook apis which are executed when events are received from azure service bus
 
**What are the interfaces exposed to debug the issues processing?**
When a task fails you can use the download api from your plugin to get all the files generated by the running applicaton. This should be enough to debug any issues that you might have encountered during running of your script. If you want to add custom debuging information then you can do so in your application, and also you can customize the event hooks to log them in the db.

**Should we still retain Jet if we can run R in batch?**
We would not want to retain jet for soe of the following reasons:
    * Mainly because we would not want to maintain and support 2 solutions. While jet works best with running analytics for reporting, batch can solve for same and even more.
    * Jet requires the consuming app to write all the queing logic
    * Jet is not event driven, relies on quartz. It is showing its age where the queued jobs sometimes do not fire and needs studio to be restarted
    * Jet is unable to stop an started process which wastes resource when user want to re-run analytics while in batch you can cancel a running task as well.

**Is using Event Hub as a messaging infrastructure a better solution compared to event service bus?**
We are discussing this topic with the Microsft folks and we should come to conclusion very soon on proper use cases for both. Having said that there are few advantages I can think of when using event hub(as diagnostic for batch account)
    * If we want to use service bus, then the application being run would be responsible for sending all the events, the basic ones being started, completed, failed. If for any reason the application goes in exception mode and these events are not fired then the task would go in "stuck" mode. Now we always argue that the application can have good exception handling, and in most case that might be true, but in the few cases where the application didnot handle the exception or for whatever reason would not want to consume service bus then event hub can be a good primary or fallback mechanism. 
    * Sari found this good read on it: http://microsoftintegration.guru/2015/03/03/azure-event-hubs-vs-azure-messaging/

**Why do we need to creating another Mongo DB to capture status?**
We were using it to store the task state. We are now leveraging batch storage table for it so no more extra db configuration needed as well since everything is contained within batch account.

**I am not clear to me on how to use this solution if I have a new long running tasks that I want to process.**
Please go through the documentation and see if that answers your questions, if it doesn't then feel free to shoot an email to deewendra.shresth@nielsen.com/sari.amitay@nielsen.com/sharath.pullapa@nielsen.com and we will make sure to address your questions. There is also link to the video on us trying to take initial jab at explaining/setting up batch (https://drive.google.com/file/d/1RkuO-aJ792aHAL03CJVoYLljRQu6GqRy/view?usp=sharing)

**This solution seems catered to run R runs only, which is something that we already have as part of the Jet service.**
Definitely not, it just happened that the first issues we wanted to tackle was around executing R. Actually it all stemmed from a requirement where we wanted to generate DOE(again R script) which could run for 3 hrs or more and we did not want to steal resources from jet. While we were discussing for solving for it, we went into  broader conversation of solving for any long running task(as indicated by Tim) and we chose this path after consulting with the microsoft folks as well.

**I am also not sure of the division of responsibility when it comes to running a process. Which component is responsible for what tasks.** 
A service requesting a task execution should be responsible for it, for eg if studio is requesting a process run then it would define what needs to be done when the state of the process/task changes - and these are captured in the plugin mechanism. This would be the normal scenario for most cases, having said that we do have a capability so that a service can also act on the process created by different service for eg: studio runs R on QP study, but  reporting may want to listen to the state of that task and upload the files once it completes.

**I have a long running task that I want to leverage batch for, what do I need to do?**

The first step is to indetify what takes the big chunk of your process/task. The basic working of batch is very similar to a normal function execution: you have a "input", you have "process" that uses that input and then generate "output". In some cases generating input itself can be a taxing process, in that case I would recommend that generating  the input also be part of the "process" so that you are offload all the heavy lifting task to the batch.

Once you have identified the 3 units, in the azure batch account side you would have to do:
* first thing to do is create an executable application that encapsulated your process and then upload it to the batch account. Lets call this application "my-batch-application".
* define a unique job under which you would want to execute the application. This is important piece because this will be tied to the plugin you would create later so that we can consume events generated by running tasks in that job. 
* create a pool that is best suited for the volume of your application run that you are anticipating. The big part of creating pool is also defining the VM, if you application needs any provisioning/setup that is time consuming then we would recommend to create an VM image that already has the setup or else chose the VM with your choice of OS.

For writing plugin follow the instructions provided [here](./.././writing-plugins)

